{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Random Forest 개념 정리.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPmJanK3ViDLzGS670fgMv0"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Vvyr-zrWanXx"},"source":["# Random Forest 개념 정리\n","\n","## **Random Forest**란?\n"]},{"cell_type":"markdown","metadata":{"id":"d_J1ADU3cVO1"},"source":["**여러 개의 Decision tree를 형성**하고, 새로운 데이터 포인트를 각 트리에 동시에 통과시키며, 각 트리가 분류한 결과에서 **투표를 실시하여 가장 많이 득표한 결과를 최종 분류 결과로 선택**하는 방식의 분류 알고리즘"]},{"cell_type":"markdown","metadata":{"id":"ONOfbgrMbwjH"},"source":["Ref) 전체적인 내용은 아래 링크의 자료를 참고하였다.  (해당 자료만 읽어도 RF의 개념은 충분히 이해할 수 있다.)\n","https://eunsukimme.github.io/ml/2019/11/26/Random-Forest/\n","\n"]},{"cell_type":"markdown","metadata":{"id":"oiGb8HnDcFC-"},"source":["### **1. Decision tree**\n"]},{"cell_type":"markdown","metadata":{"id":"7GpF5uBpcScj"},"source":["RF는 Decision tree를 coponent tree로 사용하는 모델로, decidion tree가 여럿 모여 숲 Forest를 이룬다고 볼 수 있다.\n","\n","decision tree는 훈련 데이터에 overfitting 되는 경향이 있는데, RF 모델은 이러한 decision tree의 단점을 해결하였다. \n","\n","decision tree는 데이터 분할 시 정보 이득이 높은 feature를 분류 기준으로 선택하는 방식으로 모델을 생성한다. 즉 **정보 이득이 높은 방향으로 학습을 진행**한다. 정보 이득의 개념을 이해하기 위해서는 먼저 불순도의 개념을 알아야 한다.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"-R2RlJv-d2hF"},"source":["Ref) Decision tree\n","\n","https://eunsukimme.github.io/ml/2019/11/10/Decision-Tree/\n","\n","https://m.blog.naver.com/PostView.nhn?blogId=ehdrndd&logNo=221158124011&proxyReferer=https:%2F%2Fwww.google.com%2F"]},{"cell_type":"markdown","metadata":{"id":"ns26T5S9dhby"},"source":["#### **Gini Impurity (지니 불순도)**"]},{"cell_type":"markdown","metadata":{"id":"JTmlVBASdnPR"},"source":["'불순도'란 집합에 얼마나 이질적인 것이 섞였는지에 대한 것으로, 불순도 지표로는 지니 불순도와 엔트로피 등이 있으며 여기서는 지니 불순도에 대해 다룬다.\n","\n","**Gini Impurity는 1에서 그룹에 속한 데이터의 각 label이 차지하는 비율의 제곱을 빼는 것**이다. 따라서 어떤 하나의 그룹 내에 한 종류의 label만 존재한다면 해당 그룹의 Gini Impurity는 0이 된다.\n","\n","Impurity 불순도 값이 낮을수록 더 'pure'하다. 또는 데이터가 잘 분류되었다고 표현한다."]},{"cell_type":"markdown","metadata":{"id":"s3JrxV6Ne8Ie"},"source":["#### **Information Gain (정보 획득)**"]},{"cell_type":"markdown","metadata":{"id":"yYOIBIvRfAPB"},"source":["Decision tree에서 데이터 분할의 기준이 될 Column, 속성은 Information Gain의 크기를 비교하여 결정한다.\n","\n","Information Gain은 데이터 분할 전의 불순도로부터 분할 후 각 그룹의 불순도를 모두 빼는 방식으로 구한다.\n","\n","**Information Gain = 데이터 분할 전의 불순도 - [데이터 분할 후 각 그룹의 불순도]**\n","\n","이렇게 구한 Information Gain은 1. 음수 값이 될 수 있고 2. 분할 후 데이터가 더 많이 포함된 그룹이 더 큰 의미를 가진다는 사실이 반영되지 않았다는 문제점이 있다. 이를 보완한 Information Gain이 바로 Weighted Information Gain이다.\n","\n","**Weighted Information Gain**은 분할 후 데이터 수가 더 많은 그룹에 더 높은 중요도를 부여하여 계산하는 방식으로, 분할 후 각 그룹의 불순도와 분할 전후 그룹의 데이터 수 비율을 곱하여 빼는 방식이다.\n","\n","**Weighted Information Gain = 데이터 분할 전의 불순도 - [ (분할  후 각 그룹 데이터 수 / 분할 전 데이터 수 ) * 분할 후 각 그룹의 불순도]**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"MJoC4BIWkK9U"},"source":["#### __Split의 기준 (분할의 기준)__\n"]},{"cell_type":"markdown","metadata":{"id":"ZdSeD8PGmP80"},"source":["   \n","하나의 Tree Node에서 Split을 하기 전, 해당 그룹을 여러 기준으로 분할 하여 Information Gain을 구한 후에 Information Gain이 가장 큰 기준을 해당 노드의 분할 기준으로 삼는다.\n","\n","데이터가 가진 Column이 모두 범주화된 속성이라면 각 Column을 돌아가며 기준으로 삼고 분할 해보는 것은 어렵지 않다. 그러나 연속된 속성 값이라면, 하나의 Column이라도 여러가지 분할 방식을 가지게 된다.\n","\n","연속형 변수를 Split 하는 방식에는 크게 세 가지가 있다.\n","1. 전체 데이터를 모두 기준점으로 삼는 방식\n","2. 중위수, 사분위수를 기준점으로 삼는 방식\n","3. Label의 Class가 바뀌는 지점를 기준점으로 삼는 방식\n","\n","\n","\n","Ref) Split of Continuous Features \n","\n","https://blog.naver.com/koys007/220862409263\n","\n","https://leedakyeong.tistory.com/entry/%EC%9D%98%EC%82%AC%EA%B2%B0%EC%A0%95%EB%82%98%EB%AC%B4Decision-Tree-%EB%8F%85%EB%A6%BD%EB%B3%80%EC%88%98%EA%B0%80-%EC%97%B0%EC%86%8D%ED%98%95-%EC%9D%BC-%EB%95%8C"]},{"cell_type":"markdown","metadata":{"id":"ant7RHVAiZ23"},"source":["### **2. Bagging**"]},{"cell_type":"markdown","metadata":{"id":"BzAC8oOtifLi"},"source":["#### Ensemble Model (앙상블 모델)\n","동일한 학습 알고리즘을 사용하여 여러 모델을 학습시키고, 이를 기반으로 예측을 결합하여 최종 결과를 내는 모델\n"]},{"cell_type":"markdown","metadata":{"id":"ps6Ngomlla2L"},"source":["#### Bagging\n","\n","Ensemble Model에는 각 모형을 parallel하게 training시키며 각 모형을 하나의 데이터셋의 bootstrap sample에 의해 학습되는 __Bagging__과, 각 모형을 sequential하게 training시키며 각 모형은 이전 모형에 의해 발생한 Error로부터 학습해나가는 __Boosting__이 있고, __RF 모델에서는 Bagging 방식을 사용__한다.\n","\n","__Bagging__의 Bootstrap sample이란 하나의 모집단, 표본집단을 더 작은 집단으로 복원추출하는 것을 의미하고, 이러한 방식은 주로 시간과 비용이 많이 드는 데이터 수집을 스스로 해결하고자 할 때, 하나의 표본 집단으로 모델링을 위해 적용한다. RF 모델에서는 training set의 부분 집합을 활용하여 tree를 형성하는 것이라고 보면 된다. 즉 각 coponent decision tree는 각기 다른 데이터셋으로 학습하며 이때 데이터셋은 모두 training set의 부분집합이고, 부분집합을 만들 때는 데이터의 중복을 허용한다.\n","\n","Bagging = Bootstrap Aggregating (샘플을 여러 번 뽑고, 그 결과를 집계한다.)"]},{"cell_type":"markdown","metadata":{"id":"PGaHr-1-g7Iw"},"source":["Ref) 앙상블 머신러닝 모델, Bagging, RF에 대한 간략한 언급\n","https://be-favorite.tistory.com/2"]},{"cell_type":"markdown","metadata":{"id":"K89mumcvlryI"},"source":["#### Bagging Feature\n","\n","Component tree 형성 시, 데이터셋에만 Bagging 방식으로 변화를 주는 것이 아니라, 분류의 기준이 되는 Feature를 선택하는데 있어서도 같은 방식의 변화를 준다. 즉 분류 시, Feature를 선택할 때도 데이터셋 분류처럼, 기존에 존재하는 Featrue의 부분집합을 활용한다.\n","\n","일반적으로 M개의 feature가 존재할 때, $ \\sqrt M $개의 feature를 임의로 선택하고, 그 중 Information Gain이 가장 높은 feature를 선택하여 데이터를 분류하는데, 이 과정을 Tree가 만들어질 때까지 매 분할마다 반복한다.\n"]},{"cell_type":"markdown","metadata":{"id":"rbMnDOSOnPXB"},"source":["---\n"]},{"cell_type":"markdown","metadata":{"id":"SuwMoEWqnYB0"},"source":["## **3. Random Forest**\n","\n","Random Forest 모델은 위와 같은 규칙들을 바탕으로 Bagging을 적용하여 여러 Decision Tree를 만든다. 임의의 데이터를 RF 모델에 넣으면, 데이터는 각 Tree에 전달되고 각 Tree는 분류 결과를 내놓는다. 데이터는 분류 결과 수에 따라, 가장 많은 득표를 얻은 class로 분류된다.\n","\n","이때 Random Forest의 component인 Decision Tree의 수는 조정할 수 있다."]}]}